<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>sen</title>
  
  
  <link href="http://senye.ink/atom.xml" rel="self"/>
  
  <link href="http://senye.ink/"/>
  <updated>2021-01-17T10:16:37.967Z</updated>
  <id>http://senye.ink/</id>
  
  <author>
    <name>zhangsen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据编程</title>
    <link href="http://senye.ink/2021/01/17/dashuju/"/>
    <id>http://senye.ink/2021/01/17/dashuju/</id>
    <published>2021-01-17T10:03:02.624Z</published>
    <updated>2021-01-17T10:16:37.967Z</updated>
    
    <content type="html"><![CDATA[<p>1.分布式文件系统是如何实现较高水平的扩展的？<br> 分布式文件系统把文件分布存储到多个计算机节点上，成千上万的计算机节点构成计算机集群。<br>2.试述HDFS中的名称节点和数据节点的具体功能。<br>名称节点：负责文件和目录的创建删除和重命名等，管理数据节点和文件块的映射关系。<br>数据节点：负责数据的存储和读取。<br>3.在分布式文件系统中，中心节点的设计至关重要，请阐述HDFS是如何减轻中心节点的负担的。<br>名称节点不参与数据的传输。<br>4.试述HDFS的冗余数据保存策略。<br>HDFS采用多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分不到不同的数据节点上。<br>5.请阐述HBase和传统关系数据库的区别。<br> 主要体现在6个方面。<br>主要方面    关系数据库    HBase<br>数据类型    关系模型 丰富的数据类型和存储方式    简单的数据模型 数据存储为未经解释的字符串<br>数据操作    丰富 插入 删除 更行 查询等 多表连接    不存在复杂的表与表之间的关系 仅插入 查询 删除 清空等<br>存储模式    基于行模式存储    基于列存储<br>数据索引    针对不同列构建复杂的多个索引，提高数据访问性能    索引是行键<br>数据维护    更行操作用最新的当前值替换记录中原来的旧值    更新操作不删除数据旧的版本<br>可伸缩性    难实现横向扩展，纵向扩展的空间有限    水平扩展灵活 轻易的通过在集群中增加或者减少硬件数量来实现性能的伸缩<br>6.HBase有哪些类型的访问接口？<br>  HBase提供了Native Java API , HBase Shell , Thrift Gateway , REST GateWay , Pig , Hive 等访问接口。<br>7.HBase中的分区是如何定位的？<br>通过构建的映射表的每个条目包含两项内容，一个是Regionde 标识符，另一个是Region服务器标识，这个条目就标识Region和Region服务器之间的对应关系，从而就可以知道某个Region被保存在哪个Region服务器中。<br>8.试述HBase的三层结构中各层次的名称和作用。<br>层次    名称    作用<br>第一层    Zookeeper文件    记录了-ROOT-表的位置信息<br>第二层    -ROOT-表    记录了.META.表的Region位置信息-ROOT-表只能有一个Region。通过-ROOT-表，就可以访问.META.表中的数据<br>第三层    .META.表    记录了用户数据表的Region位置信息，.META.表可以有多个Region，保存了HBase中所有请阐述在HBase三层结构下，客户端是如何访问到数据的。<br>9.试述HBase系统基本架构以及每个组成部分的作用。<br>（1）客户端<br>客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程<br>（2）Zookeeper服务器<br>Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题<br>（3）Master<br>主服务器Master主要负责表和Region的管理工作：管理用户对表的增加、删除、修改、查询等操作；实现不同Region服务器之间的负载均衡；在Region分裂或合并后，负责重新调整Region的分布；对发生故障失效的Region服务器上的Region进行迁移<br>（4）Region服务器<br>Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1.分布式文件系统是如何实现较高水平的扩展的？&lt;br&gt; 分布式文件系统把文件分布存储到多个计算机节点上，成千上万的计算机节点构成计算机集群。&lt;br&gt;2.试述HDFS中的名称节点和数据节点的具体功能。&lt;br&gt;名称节点：负责文件和目录的创建删除和重命名等，管理数据节点和文件块的</summary>
      
    
    
    
    
    <category term="大数据编程 方面的知识要点" scheme="http://senye.ink/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BC%96%E7%A8%8B-%E6%96%B9%E9%9D%A2%E7%9A%84%E7%9F%A5%E8%AF%86%E8%A6%81%E7%82%B9/"/>
    
  </entry>
  
  <entry>
    <title>Spark</title>
    <link href="http://senye.ink/2021/01/17/spark/"/>
    <id>http://senye.ink/2021/01/17/spark/</id>
    <published>2021-01-17T09:20:24.804Z</published>
    <updated>2021-01-17T09:23:21.947Z</updated>
    
    <content type="html"><![CDATA[<p>1.4v：数据量大、数据类型繁多、处理速度快、价值密度低<br>2.技术层面：数据采集与预处理、数据存储和管理、数据处理和分析、数据可视化、数据安全和隐私保护<br>3.大数据计算模式：批处理mapreduce，流计算storm，图计算pregel，查询分析计算<br>4.Spark 特点 运行速度快，易使用，通用性，运行模式多样<br>5.Yarn负责集群资源调度管理的组件 提高集群利用率，共享底层存储<br>6.Hive基于hadoop的数据仓库工具，可以对hadoop文件中的数据集进行数据整理、特殊查询、分析处理<br>7.hadoop 批处理 spark基于内存（发展早，社区活跃） flink约等于spark  beam<br>8../sbin/start-dfs.sh<br>9.Val 变量名称：数据类型=初始值 不可变<br>10.readInt 导入 scala.io.StdIn<br>11.print（s””）不支持格式化，f支持 等效format<br>12.for(i&lt;- 1 to 5 if i%2=0)<br>13.Array数组是一种可变，可索引，元素具有相同数据类型的数据集合，从0<br>14.Tuple对多个不同类型对象的一种简单封装，从1开始的索引<br>15.容器：列表list从0索引，映射map键值，集合set没有<br>16.序列sequence 元素按特定顺序访问容器<br>17.列表list共享相同类型的不可变的对象序列 head列表第一个元素值，tail除1的新列表  range特殊带索引的不可变数字等差序列，从给定起点到终点所有值<br>18.set不重复元素的容器，包括可变集和不可变<br>19.map，键是唯一的，值不一定<br>20.类用关键字class声明<br>def 方法名（参数列表）：结果类型={方法体}<br>21.类成员默认共有，private本类型和嵌套 protected本类和继承<br>getter  value  setter  value_=<br>22.主构整个类的定义主体就是类的构造器，类方法以外的语句都在构造过程中执行 辅构this定义，返回类型为unit，第一个表达式this（参）已定义的辅或主<br>23.单例对象 伴生和孤立什么是伴生对象，为什么设计apply<br>一个单例对象和它同名类一起出现 伴生对象的方法只能通过伴生对象调用<br>Apply保持对象和函数之间使用的一致性，接受构造参数变成对象<br>Unapply接受一个对象从中提取值<br>24.abstract定义抽象类 类层级结构 any anyval anyRef null nothing<br>25.Option抽象类 子类some有值 none没<br>26.特质 实现代码复用，实现接口  trait定义特质 extends with混入类<br>27.定义函数作为某个类或对象的成员，定义为def。。<br>28.头等公民，类型明确函数接收参数、类型、返回结果，值函数的具体实现<br>29.高阶函数 函数包含其他函数作为其参数或返回结果为一个函数<br>30.Foreach 遍历 map映射filter过滤 reduce规约<br>拆分操作partition、groupedby、grouped、sliding<br>31.Spark的特点：运行速度快，容易使用，通用性，运行模式多样<br>32.大数据包括的类型：批量数据处理，交互式查询，流数据处理<br>33、Spark生态系统包含spark core；spark  sql；spark streaming；mlib；graphx</p><p>关于RDD<br>RDD:是弹性分布式数据集,是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型;<br>DAG:是有向无环图的简称，反映RDD之间的依赖关系<br>阶段:是作业的基本调度单位，一个作业会分为组任务，每组任务被称为“阶段”，或者也被称为“任务集”。<br>分区：RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区。<br>​窄依赖：一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区，无shuffle，平行<br>​宽依赖：一个父RDD的一个分区对应一个子RDD的多个分区，有shuffle， 任务</p><p>​行动”( Action )和“转换” ( Transformation )两种类型、前者用于执行计算并指定输出的形式，后者指定RDD之间的相互依赖关系。两类操作的主要区别是转换操作(比如map、filter、 groupBy、join 等)接受RDD并返回RDD而行动操作(比如count、collect等)接受RDD但是返回非RDD (即输出一个值或结果)。</p><p>关于编程<br>1、统计文本文件中单词出现的频率<br>方法一：<br>“Hello world’’<br>‘’hello china’’<br>scala&gt;Val list1=List(“Hello world”,”Hello china”)<br>scala&gt;Val list2=list1.flatmap(s=&gt;s.split(“ ”))<br>H,e,l,l,o , ,w,o,r,l,d  错<br>Hello，world，hello，china对<br>分析：Hello world—-〉s—–〉对s按照空格进行拆分—-〉得到结果<br>List2：List(“Hello”,”world”,”Hello”,”china”)<br>scala&gt;Val list3=list2.map(x=&gt;(x,1))<br>List3:List(&lt;Hello ,1&gt;,&lt;Hello ,1&gt;,&lt;world,1&gt;,&lt;china,1&gt;),reduce麻烦<br>scala&gt;Val list4=list3.groupBy(x=&gt;x.<em>1)<br>Map(Hello-&gt;List(&lt;Hello,1&gt;,&lt;Hello,1&gt;) ,world-&gt;List(&lt;world,1&gt;) ,china-&gt;List(&lt;china,1) )<br>scala&gt;Val list5=list4.map(x=&gt;(x._1,(x._2).size))<br>得到&lt;hello,2&gt;，&lt;world,1&gt;，&lt;china,1&gt;<br>1,2,3,4,5<br>List(1,2,3,4,5).groupBy(x=&gt;x%3)<br>Map(2-&gt;List(2,5), 1-&gt;List(1,4), 0-&gt;List(3))<br>方法二：<br>import java.io.File<br>import scala.io.Source<br>import collection.mutable.Map<br>object WordCount{<br>     def main(args:Array[String]){<br>        val dirfile=new File(“testfiles”)<br>        val files=dirfile.listFile<br>        val results=Map.empty[String,Int]<br>         for(file&lt;-files){<br>           val data=Source.fromFile(file)<br>           val strs=data.getLines.flatMap{s=&gt;s.split(“  “)}<br>           strs foreach{ word=&gt;<br>                   if (results.contains(word))<br>                   results(word)+=1 else results(word)=1<br>                       }   }<br>           results foreach{case (k,v)=&gt;println(s”$k,$v”)}<br>           }   }<br>统计hello world的行数<br>import  org.apache.spark.sparkContext<br>import  org.apache.spark.sparkContext.</em><br>import  org.apache.spark.sparkConf<br>object HelloWorld{<br>   def main (args:Array[String]){<br>   val conf=new SparkConf().setAppName(“Hello World”).setMaster(“local[2]”)<br>val sc=new SparkContext(conf)<br>val fileRDD=sc.textFile(“hdfs://localhost:9000/examplefile”)<br>val filterRDD=fileRDD.filter(_.contains(“Hello World”))<br>filterRDD.cache()<br>filterRDD.count()    }    }</p><p>2、（1） 该系总共有多少学生；<br>val lines = sc.textFile(“file:///usr/local/spark/sparksqldata/Data01.txt”)<br>val par = lines.map(row=&gt;row.split(“,”)(0))<br>val distinct_par = par.distinct() //去重操作<br>distinct_par.count //取得总数<br>（2） 该系共开设来多少门课程；<br>val lines = sc.textFile(“file:///usr/local/spark/sparksqldata/Data01.txt”)<br>val par = lines.map(row=&gt;row.split(“,”)(1))<br>val distinct_par = par.distinct()<br>distinct_par.count<br>（3） Tom 同学的总成绩平均分是多少；<br>val lines = sc.textFile(“file:///usr/local/spark/sparksqldata/Data01.txt”)<br>val pare = lines.filter(row=&gt;row.split(“,”)(0)==”Tom”)<br>pare.foreach(println)<br>Tom,DataBase,26<br>Tom,Algorithm,12<br>Tom,OperatingSystem,16<br>Tom,Python,40<br>Tom,Software,60<br>pare.map(row=&gt;(row.split(“,”)(0),row.split(“,”)(2).toInt)).mapValues(x=&gt;(x,1)).reduceByKey((x,y ) =&gt; (x._1+y._1,x._2 + y._2)).mapValues(x =&gt; (x._1 / x._2)).collect()<br>（4） 求每名同学的选修的课程门数；<br>val lines = sc.textFile(“file:///usr/local/spark/sparksqldata/Data01.txt”)<br>val pare = lines.map(row=&gt;(row.split(“,”)(0),row.split(“,”)(1)))<br>pare.mapValues(x =&gt; (x,1)).reduceByKey((x,y) =&gt; (“ “,x._2 + y._2)).mapValues(x =&gt;<br>x._2).foreach(println)<br>（5） 该系 DataBase 课程共有多少人选修；<br>val lines = sc.textFile(“file:///usr/local/spark/sparksqldata/Data01.txt”)<br>val pare = lines.filter(row=&gt;row.split(“,”)(1)==”DataBase”)<br>pare.count<br>（6） 各门课程的平均分是多少；<br>val lines = sc.textFile(“file:///usr/local/spark/sparksqldata/Data01.txt”)<br>val pare = lines.map(row=&gt;(row.split(“,”)(1),row.split(“,”)(2).toInt))<br>pare.mapValues(x=&gt;(x,1)).reduceByKey((x,y) =&gt; (x._1+y._1,x._2 + y._2)).mapValues(x =&gt; (x._1/ x._2)).collect()<br>（7）使用累加器计算共有多少人选了 DataBase 这门课。<br>val lines = sc.textFile(“file:///usr/local/spark/sparksqldata/Data01.txt”)<br>val pare = lines.filter(row=&gt;row.split(“,”)(1)==”DataBase”).map(row=&gt;(row.split(“,”)(1),1))<br>val accum = sc.longAccumulator(“My Accumulator”)<br>pare.values.foreach(x =&gt; accum.add(x))<br>accum.value </p><p>对于两个输入文件 A 和 B，编写 Spark 独立应用程序，对两个文件进行合并，并剔除其<br>中重复的内容，得到一个新文件 C。<br>（１）假设当前目录为/usr/local/spark/mycode/remdup，在当前目录下新建一个目录 mkdir -p<br>src/main/scala，然后在目录/usr/local/spark/mycode/remdup/src/main/scala 下新建一个<br>remdup.scala，复制下面代码；<br>object RemDup {<br>def main(args: Array[String]) {<br>val conf = new SparkConf().setAppName(“RemDup”)<br>val sc = new SparkContext(conf)<br>val dataFile = “file:///home/charles/data”<br>val data = sc.textFile(dataFile,2)<br>val res = data.filter(_.trim().length&gt;0).map(line=&gt;(line.trim,””)).partitionBy(new<br>HashPartitioner(1)).groupByKey().sortByKey().keys<br>res.saveAsTextFile(“result”) } }<br>（２）在目录/usr/local/spark/mycode/remdup 目录下新建 simple.sbt，复制下面代码：<br>name := “Simple Project”<br>version := “1.0”<br>scalaVersion := “2.11.8”<br>libraryDependencies += “org.apache.spark” %% “spark-core” % “2.1.0”<br>（３）在目录/usr/local/spark/mycode/remdup 下执行下面命令打包程序<br>$ sudo /usr/local/sbt/sbt package<br>（４）最后在目录/usr/local/spark/mycode/remdup 下执行下面命令提交程序<br>$ /usr/local/spark2.0.0/bin/spark-submit –class “RemDup”<br>/usr/local/spark2.0.0/mycode/remdup/target/scala-2.11/simple-project_2.11-1.0.jar<br>（５）在目录/usr/local/spark/mycode/remdup/result 下即可得到结果文件。</p><p>每个输入文件表示班级学生某个学科的成绩，每行内容由两个字段组成，第一个是学生<br>名字，第二个是学生的成绩；编写 Spark 独立应用程序求出所有学生的平均成绩，并输出到一个新文件中。<br>（１）假设当前目录为/usr/local/spark/mycode/avgscore，在当前目录下新建一个目录 mkdir -p<br>src/main/scala，然后在目录/usr/local/spark/mycode/avgscore/src/main/scala 下新建一个<br>avgscore.scala<br>object AvgScore {<br>def main(args: Array[String]) {<br>val conf = new SparkConf().setAppName(“AvgScore”)<br>val sc = new SparkContext(conf)<br>val dataFile = “file:///home/charles/data”<br>val data = sc.textFile(dataFile,3)<br>val res = data.filter(_.trim().length&gt;0).map(line=&gt;(line.split(“ “)(0).trim(),line.split(“<br>“)(1).trim().toInt)).partitionBy(new HashPartitioner(1)).groupByKey().map(x =&gt; {<br>var n = 0<br>var sum = 0.0<br>for(i &lt;- x._2){<br>sum = sum + i<br>n = n +1 }<br>val avg = sum/n<br>val format = f”$avg%1.2f”.toDouble<br>(x._1,format) })<br>res.saveAsTextFile(“result”) } }<br>（２）在目录/usr/local/spark/mycode/avgscore 目录下新建 simple.sbt，复制下面代码：<br>name := “Simple Project”<br>version := “1.0”<br>scalaVersion := “2.11.8”<br>libraryDependencies += “org.apache.spark” %% “spark-core” % “2.1.0”<br>（３）在目录/usr/local/spark/mycode/avgscore 下执行下面命令打包程序<br>$ sudo /usr/local/sbt/sbt package<br>（４）最后在目录/usr/local/spark/mycode/avgscore 下执行下面命令提交程序<br>$ /usr/local/spark2.0.0/bin/spark-submit –class “AvgScore”<br>/usr/local/spark2.0.0/mycode/avgscore/target/scala-2.11/simple-project_2.11-1.0.jar<br>（５）在目录/usr/local/spark/mycode/avgscore/result 下即可得到结果文件。 </p><p>Scala词频统计<br>val list1=List(“Hello world”,”Hello china”)<br>Val list2=list1.flatmap(s=&gt;s.split(“ ”))<br>Val list3=list2.map(s=&gt;(s,1))<br>Val list4=list3.groupBy(x=&gt;x._1)<br>Val list5=list4.map(x=&gt;(x._1,(x._2).size))</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1.4v：数据量大、数据类型繁多、处理速度快、价值密度低&lt;br&gt;2.技术层面：数据采集与预处理、数据存储和管理、数据处理和分析、数据可视化、数据安全和隐私保护&lt;br&gt;3.大数据计算模式：批处理mapreduce，流计算storm，图计算pregel，查询分析计算&lt;br&gt;4.</summary>
      
    
    
    
    
    <category term="关于spark的一些基础知识" scheme="http://senye.ink/tags/%E5%85%B3%E4%BA%8Espark%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Maven</title>
    <link href="http://senye.ink/2021/01/17/maven/"/>
    <id>http://senye.ink/2021/01/17/maven/</id>
    <published>2021-01-17T09:10:59.641Z</published>
    <updated>2021-01-17T09:17:21.314Z</updated>
    
    <content type="html"><![CDATA[<p>1、解决依赖管理冲突的原则 路径优先原则 ，相同路径，第一声明者优先原则    。</p><p>2.Maven是Apache下的一个开源项目，它是一个项目管理工具，它用于对java项目进行   项目构建   、    依赖管理  及  项目信息管理     。</p><p>3.一个项目的生理周期构建过程为：清理、编译、测试、报告、打包、部署。 </p><p>4.Maven包含了一个项目对象模型，一组标准集合，一个项目生命周期，一个依赖管理系统，和用来运行定义在生命周期阶段中插件目标的逻辑。</p><p>5.maven管理项目生命周期过程都是基于插件完成的</p><p>6.设置系统环境变量：M2_HOME path环境变量：%M2_HOME%\bin</p><p>7.项目对象模型: 通过pom.xml定义项目的坐标、项目依赖、项目信息、插件目标</p><p>8.项目依赖struts2.3.24，通过在pom.xml中定义依赖即可将struts2的jar包自动加入工程。</p><p>关于简答</p><p>1.简述什么是Maven坐标，由哪些标签组成，分别代表什么，为什么要使用坐标。</p><p>（1）Maven坐标是可以标识平面内的一个点</p><p> 由groupid 定义maven项目的隶属目录</p><p>artifactID 定义实际项目中的一个模块</p><p>version 定义该项目的版本号</p><p>packaging  定义该项目的打包方式</p><p>（坐标决定了Maven的唯一性）</p><p>（2）为什么要使用坐标</p><p>Maven世界中有大量的构建，我们需要用一个唯一标识一个构建的统一规范</p><p>有了统一规范，就可以把查找工作交给机器来做</p><p>2.简述本地仓库与远程仓库的不同。</p><p>本地仓库：一个用户只有一个本地仓库，需要从中央仓库下载，资源存放在本地仓库</p><p>远程仓库：1中央仓库，maven的默认远程仓库</p><p>​       2 .私服，一个特殊的远程仓库，用于在局域网内的仓库</p><p>​                3.镜像：用来替代中央仓库，速度比一般仓库快</p><p>​    </p><p>3.简述Maven的作用和意义。</p><p><strong>–</strong> <strong><em>\</em>拥有约定，知道你的代码在哪里，放到哪里去**</strong></p><p><strong>–</strong> <strong><em>\</em>拥有一个生命周期，例如执行**</strong> <strong><em>\</em>mvn install**</strong> <strong><em>\</em>就可以自动执行编译，测试，打包等构建过程**</strong></p><p><strong>–</strong> <strong><em>\</em>只需要定义一个*****</strong>*pom.xml,*<strong>**</strong>*然后把源码放到默认的目录，**<strong><strong><em>\</em>Maven***</strong></strong>*帮你处理其他事情****</p><p><strong>–</strong> <strong><em>\</em>拥有依赖管理，仓库管理**</strong></p><p>4.Maven项目约定由哪几个目录结构组成，分别存放哪些文件。</p><p>Src/main/java 存放项目的.java文件</p><p>Src/main/resource 存放项目的资源文件</p><p>Src/test/java 存放所有测试.java文件</p><p>Src/test/resource 存放项目的测试资源文件</p><p>Target 项目输出位置</p><p>Pom.xml</p><p>5.三大生命周期</p><p>clean:清理项目</p><p>default：构建项目</p><p>site：生成项目站点的</p><p>6.Maven安装目录分析：</p><p>Bin目录： Maven的运行脚本</p><p>boot目录:Maven自己的类加载器</p><p>conf 目录：含有setting.xml配置文件</p><p>lib ：Maven运行时所需的java类库、</p><p>7.系统报错说没有找到依赖，报错的原因是找不到依赖的Hello项目，HelloFriend中需要调用Hello类，并且在pom.xml中有依赖，但是在库中没有添加进来，所以会报错。故需要将Hello项目的jar包放入仓库中，执行mvn </p><p>Install命令，执行完该命令后再查看仓库，多了cn的文件夹。</p><p>8.scope 用来控制依赖和编译，测试，运行的classpath的关系 </p><p>依赖关系：</p><p>1.compile： 默认编译依赖范围。对于编译，测试，运行三种classpath都有效</p><p>2.test：测试依赖范围。只对于测试classpath有效</p><p>3.provided：已提供依赖范围。对于编译，测试的classpath都有效，但对于运行无效。因为由容器已经提供，例如servlet-api（Tomcat）</p><p>4.runtime:运行时提供。例如:jdbc驱动</p><p>9.Maven项目步骤:</p><p>1 建立Hello项目同时建立Maven约定的项目目录结构</p><p>2 在项目Hello根目录建立pom.xml</p><p>3 在src/main/java/cn/rjxy/maven目录下新建文件Hello.java</p><p>4 在/src/test/java/cn/rjxy/maven目录下新建测文件HelloTest.java</p><p>5 打开cmd命令行，进入Hello项目根目录执行 mvn compile命令，查看根目录变化</p><p> cmd中继续录入mvn clean命令，然后再次查看根目录变化</p><p> cmd中录入mvn clean compile命令，查看根目录变化</p><p> cmd中录入mvn  clean test命令，查看根目录变化</p><p> cmd中录入mvn clean package命令，查看根本录变化</p><p>6新建第二个项目模块hellofriend目录及约定的目录结构</p><p>7在项目hellofriend根目录建立pom.xml</p><p>8在src/main/java/cn/rjxy/maven目录下新建文件hello friend.java</p><p>9在/src/test/java/cn/rjxy/maven目录下新建测试文件hellofriendtest.java</p><p>10 在hellofriend目录下执行命令 mvn package</p><p>11 需要重新构建hello第一个项目并将jar包放入仓库中，在命令行根目录下执行 mvn clean install</p><p>12 重新在hello friend 目录下执行命令mvn package </p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、解决依赖管理冲突的原则 路径优先原则 ，相同路径，第一声明者优先原则    。&lt;/p&gt;
&lt;p&gt;2.Maven是Apache下的一个开源项目，它是一个项目管理工具，它用于对java项目进行   项目构建   、    依赖管理  及  项目信息管理     。&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
    <category term="关于maven的一些基础知识" scheme="http://senye.ink/tags/%E5%85%B3%E4%BA%8Emaven%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>初来乍到</title>
    <link href="http://senye.ink/2020/10/06/firstblog-md/"/>
    <id>http://senye.ink/2020/10/06/firstblog-md/</id>
    <published>2020-10-06T11:38:21.769Z</published>
    <updated>2021-01-17T09:10:00.694Z</updated>
    
    <content type="html"><![CDATA[<p>这是我的第一篇博客，同时也是自己用hexo和github搭建的第一个网站，内心无比激动，经过两天断断续续的查csdn与实践，终于，终于，终于到了发表文章这一步了！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这是我的第一篇博客，同时也是自己用hexo和github搭建的第一个网站，内心无比激动，经过两天断断续续的查csdn与实践，终于，终于，终于到了发表文章这一步了！&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="myfirst" scheme="http://senye.ink/tags/myfirst/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://senye.ink/2020/10/05/hello-world/"/>
    <id>http://senye.ink/2020/10/05/hello-world/</id>
    <published>2020-10-05T02:59:33.802Z</published>
    <updated>2020-10-05T02:59:33.802Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
